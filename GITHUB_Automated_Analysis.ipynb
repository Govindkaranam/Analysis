{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQBFESbB/MN8fbdEbpf8Oy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Govindkaranam/Analysis/blob/main/GITHUB_Automated_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask nbformat requests gitpython torch transformers\n",
        "import os\n",
        "import shutil\n",
        "import nbformat\n",
        "import requests\n",
        "import git\n",
        "import torch\n",
        "from nbformat import read, write, validate\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Fetch user repositories from GitHub\n",
        "def fetch_user_repositories(github_url):\n",
        "    # Extract the username from the user URL\n",
        "    username = github_url.split('/')[-1]\n",
        "\n",
        "    # GitHub API endpoint to retrieve user repositories\n",
        "    api_url = f'https://api.github.com/users/{username}/repos'\n",
        "\n",
        "    # Make an API request to retrieve repository data\n",
        "    response = requests.get(api_url)\n",
        "    repos = response.json()\n",
        "\n",
        "    # Iterate through the repositories\n",
        "    for repo in repos:\n",
        "        clone_url = repo['clone_url']\n",
        "        repo_name = repo['name']\n",
        "        repo_directory = os.path.join(\"./temp\", repo_name)\n",
        "\n",
        "        # Check if the repository directory already exists\n",
        "        if os.path.exists(repo_directory):\n",
        "            print(f\"Updating repository: {repo_name}\")\n",
        "            repo = git.Repo(repo_directory)\n",
        "\n",
        "            # Pull the latest changes from the remote repository\n",
        "            origin = repo.remote(name='origin')\n",
        "            origin.pull()\n",
        "\n",
        "        else:\n",
        "            print(f\"Cloning repository: {repo_name}\")\n",
        "            # Clone the repository if it doesn't exist locally\n",
        "            git.Repo.clone_from(clone_url, repo_directory)\n",
        "\n",
        "    # Return the list of repository names\n",
        "    return [repo['name'] for repo in repos]\n",
        "\n",
        "\n",
        "# Preprocess code in repositories\n",
        "def preprocess_code(repository):\n",
        "    # Define a temporary directory to store preprocessed files\n",
        "    temp_directory = \"./temp\"\n",
        "    os.makedirs(temp_directory, exist_ok=True)\n",
        "\n",
        "    # Clone the repository locally if it doesn't already exist\n",
        "    repo_directory = os.path.join(temp_directory, repository)\n",
        "    if not os.path.exists(repo_directory):\n",
        "        clone_command = f\"git clone https://github.com/{repository}.git {repo_directory}\"\n",
        "        os.system(clone_command)\n",
        "\n",
        "    # Iterate through the repository files\n",
        "    for root, dirs, files in os.walk(repo_directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_extension = os.path.splitext(file)[1]\n",
        "\n",
        "            # Preprocess specific file types\n",
        "            if file_extension == \".ipynb\":\n",
        "                preprocess_jupyter_notebook(file_path)\n",
        "            elif file_extension == \".py\":\n",
        "                preprocess_python_file(file_path)\n",
        "\n",
        "    # Remove temporary directory\n",
        "    shutil.rmtree(temp_directory)\n",
        "\n",
        "\n",
        "# Preprocess Jupyter notebook file\n",
        "def preprocess_jupyter_notebook(file_path):\n",
        "    # Read the Jupyter notebook\n",
        "    with open(file_path, 'r') as file:\n",
        "        notebook = read(file, as_version=4)\n",
        "\n",
        "    # Normalize the notebook to add the missing ID field\n",
        "    validate(notebook)\n",
        "\n",
        "    # Remove outputs from all cells\n",
        "    for cell in notebook.cells:\n",
        "        if 'outputs' in cell:\n",
        "            cell['outputs'] = []\n",
        "\n",
        "    # Save the preprocessed Jupyter notebook\n",
        "    with open(file_path, 'w') as file:\n",
        "        write(notebook, file, version=4)\n",
        "\n",
        "\n",
        "# Preprocess Python file\n",
        "def preprocess_python_file(file_path):\n",
        "    # Read the Python file\n",
        "    with open(file_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Remove comments and whitespace\n",
        "    lines = [line for line in lines if not line.strip().startswith(\"#\")]\n",
        "\n",
        "    # Save the preprocessed Python file\n",
        "    with open(file_path, \"w\") as file:\n",
        "        file.writelines(lines)\n",
        "\n",
        "\n",
        "# Assess technical complexity using GPT\n",
        "def assess_technical_complexity(code):\n",
        "    try:\n",
        "        # Define your own GPT2 model and tokenizer or load a pre-trained model\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # Apply prompt engineering techniques to evaluate the code's technical complexity\n",
        "        prompt = \"The code provided is:\"\n",
        "        input_text = f\"{prompt} {code}\"\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "        # Set attention mask and pad token ID\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "        pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pad_token_id=pad_token_id,\n",
        "            max_length=100,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Compute the complexity score based on generated text (e.g., using LangChain)\n",
        "        complexity_score = compute_complexity_score(generated_text)\n",
        "\n",
        "        return complexity_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing technical complexity: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Compute complexity score using LangChain or other techniques\n",
        "def compute_complexity_score(text):\n",
        "    # Placeholder implementation\n",
        "    # You can replace this with your own implementation using LangChain or other techniques\n",
        "\n",
        "    # Calculate complexity score based on the provided text\n",
        "    complexity_score = len(text)  # Placeholder complexity score calculation\n",
        "\n",
        "    return complexity_score\n",
        "\n",
        "\n",
        "# Identify most technically complex repository\n",
        "def identify_most_complex_repository(repositories):\n",
        "    most_complex_repository = None\n",
        "    max_complexity_score = 0\n",
        "\n",
        "    for repository in repositories:\n",
        "        preprocess_code(repository)\n",
        "\n",
        "        # Assess the technical complexity of the code in the repository\n",
        "        code = get_repository_code(repository)\n",
        "        complexity_score = assess_technical_complexity(code)\n",
        "\n",
        "        if complexity_score is not None and complexity_score > max_complexity_score:\n",
        "            max_complexity_score = complexity_score\n",
        "            most_complex_repository = repository\n",
        "\n",
        "    return most_complex_repository\n",
        "\n",
        "\n",
        "# Get the code from the repository\n",
        "def get_repository_code(repository):\n",
        "    # Define the directory where the repository is cloned\n",
        "    repo_directory = f\"./temp/{repository}\"\n",
        "\n",
        "    # Iterate through the repository files\n",
        "    code = \"\"\n",
        "    for root, dirs, files in os.walk(repo_directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_extension = os.path.splitext(file)[1]\n",
        "\n",
        "            # Read and append code from specific file types\n",
        "            if file_extension == \".ipynb\" or file_extension == \".py\":\n",
        "                with open(file_path, \"r\") as file:\n",
        "                    code += file.read()\n",
        "\n",
        "    return code\n",
        "\n",
        "\n",
        "# Generate justification using GPT\n",
        "def generate_justification(repository):\n",
        "    try:\n",
        "        # Define your own GPT2 model and tokenizer or load a pre-trained model\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        # Craft prompts based on the assessment and the selected repository\n",
        "        prompt = f\"The repository {repository} was selected as the most technically complex because:\"\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # Set attention mask and pad token ID\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "        pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pad_token_id=pad_token_id,\n",
        "            max_length=200,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        justification = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        return justification\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating justification: {e}\")\n",
        "        return None\n",
        "\n",
        "def first():\n",
        "    user_url = input(\"Enter GitHub user URL: \")\n",
        "    repositories = fetch_user_repositories(user_url)\n",
        "    most_complex_repository = identify_most_complex_repository(repositories)\n",
        "\n",
        "    if most_complex_repository is not None:\n",
        "        justification = generate_justification(most_complex_repository)\n",
        "\n",
        "        print(\"Most Complex Repository:\", most_complex_repository)\n",
        "        print(\"Justification:\", justification)\n",
        "    else:\n",
        "        print(\"No repositories found or error occurred.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    first()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm98U_FkgVEh",
        "outputId": "806edb5e-358c-4c74-e5c1-0a7e9a982fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter GitHub user URL: https://github.com/Govindkaranam\n",
            "Cloning repository: Lead-Scoring-Case-Study\n",
            "Cloning repository: Movie_Recomendation\n",
            "Cloning repository: Number-recognition-fromimageofaVechile\n",
            "Cloning repository: pancreatic_cancer_disease_detection\n",
            "Cloning repository: Virtual_ChartGpt_Using_LangChain\n",
            "Most Complex Repository: Lead-Scoring-Case-Study\n",
            "Justification: The repository Lead-Scoring-Case-Study was selected as the most technically complex because:\n",
            "\n",
            "A high number of cases were identified that were not identified by consensus. This resulted in an overall higher likelihood of having a significant outcome.\n",
            ". We used a method called \"clustering\" (i.e., grouping cases according to their similarities). This clustering method uses a set of random factors to determine the likelihood that each case will have a positive outcome (e.g., a 1 in 50 chance that a case is a 'greater than 50%' outcome). Clustered cases are then used to identify the highest likelihood cases and to compare them to the remaining cases. The resulting results are used for a wide variety of analyses, from meta-analysis to meta, and are sometimes cited as a reason why this method can be useful for many clinical settings. It is important to note that the number and location of these cases does not necessarily mean that all cases will be found\n"
          ]
        }
      ]
    }
  ]
}